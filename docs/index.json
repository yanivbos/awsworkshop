[
{
	"uri": "https://jefferyfry.github.io/awsworkshop/10_devops_cloud/1_continuous_integration_and_delivery.html",
	"title": "Continuous Integration and Delivery",
	"tags": [],
	"description": "",
	"content": "Continuous integration and delivery (CI/CD) is the process for which your software components are built from code, integrated, tested, released, deployed and ultimately delivered to end-users. CI/CD pipelines are the software assembly line that orchestrates the building of your software. This CI/CD pipeline line requires infrastructure. Cloud computing has allowed this infrastructure to become dynamic and ephemeral. On cloud infrastructure, your CI/CD pipelines scale up and down to meet your software delivery demands. But at the same time, it saves costs by providing the right amount of cloud infrastructure just as it is needed. This is further realized by using cloud-native technologies like Kubernetes and extending across clouds and on-premise datacenters. The following are some AWS cloud technologies that CI/CD pipelines can utilize:\n EC2 instances can be used as CI/CD pipeline nodes that can be dynamically spun up and down to execute pipeline tasks. EC2 spot instances can dramatically lower costs by utilizing spare capacity nodes for pipeline tasks. EKS can provide a Kubernetes-based node pool and allow more efficient use of compute resources. AWS Outposts can allow you to span your CI/CD pipelines from your on-premise datacenter to the cloud for hybrid and migration use cases.  "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/55_workshop_setup/self_paced/account.html",
	"title": "Create an AWS account",
	"tags": [],
	"description": "",
	"content": " Your account must have the ability to create new IAM roles and scope other IAM permissions.\n  If you don\u0026rsquo;t already have an AWS account with Administrator access: create one now by clicking here\n Once you have an AWS account, ensure you are following the remaining workshop steps as an IAM user with administrator access to the AWS account: Create a new IAM user to use for the workshop\n Enter the user details:  Attach the AdministratorAccess IAM Policy:  Click to create the new user:  Take note of the login URL and save:   "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab/1_fork_lab_repo.html",
	"title": "Fork the Lab Repository",
	"tags": [],
	"description": "",
	"content": "To get started, you will need to fork this workshop repository. It contains the files, scripts and configurations that are required to complete the hands-on lab.\n Navigate to https://github.com/jefferyfry/awsworkshop. In the top-right corner of the GitHub repository page, click Fork. Wait for GitHub to complete the forking process. In your Cloud9 terminal, clone the new fork to your local git directory.\n$ git clone https://github.com/[username]/awsworkshop.git  In the next sections, you will make changes to your new repository.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/",
	"title": "JFrog DevOps Modernization Workshop",
	"tags": [],
	"description": "",
	"content": " DevOps Modernization Workshop Welcome In this workshop you will learn about the JFrog Platform and how to leverage Artifactory, XRay and Pipelines for managing your Software Development Lifecycle (SDLC) and bring DevOps to the cloud on AWS.\nLearning Objectives  Understand the roles of Artifactory, XRay and Pipelines in your software delivery life cycle (SDLC). Use Local, Remote and Virtual Repositories in Artifactory. Publish and promote Build Info. Scan your artifacts and builds for security vulnerabilities.  The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.  "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/55_workshop_setup/start_workshop.html",
	"title": "Start the Workshop...",
	"tags": [],
	"description": "",
	"content": " Getting Started To start the workshop, follow one of the following depending on whether you are\u0026hellip;\n \u0026hellip;running the workshop on your own, or \u0026hellip;attending an AWS hosted event  "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/55_workshop_setup/aws_event.html",
	"title": "At an AWS Event",
	"tags": [],
	"description": "",
	"content": " To complete this workshop, you are provided with an AWS account via the AWS Event Engine service. A 12-digit hash will be provided to you by event staff - this is your unique access code. eg:\ne8476543c00e Create AWS Account 1 . Connect to the portal by clicking the button or browsing to https://dashboard.eventengine.run/. The following screen shows up. Enter the provided hash in the text box. The button on the bottom right corner changes to Accept Terms \u0026amp; Login. Click on that button to continue.\n2 . Choose AWS Console, then Open AWS Console. This account will expire at the end of the workshop and the all the resources created will be automatically deprovisioned. You will not be able to access this account after today.\n3 . Use a single region for the duration of this workshop. This workshop supports the following regions:\n us-west-2 (US West - Oregon)  Please select US West (Oregon) in the top right corner.\nNext step Once you have completed the step above, you can leave the AWS console open. You can now move to the Create a workspace section.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/10_devops_cloud/10_binary_repository_management.html",
	"title": "Binary Repository Management",
	"tags": [],
	"description": "",
	"content": "A Binary Repository Manager is a software hub that simplifies the development process for different teams across an organization by helping them to collaborate on building coherent and compatible software components. It does this by centralizing the management of all the binary artifacts generated and used by the organization, thereby overcoming the incredible complexity arising from diverse binary artifact types, their position in the overall workflow and the set of dependencies between them.\nSome of the many benefits of using a Binary Repository Manager are:\n Reliable and consistent access to remote artifacts. Reduced network traffic and optimized builds. Tight integration with build ecosystems. Custom handling of artifacts to comply with any organization’s requirements. Security and access control to artifacts and repositories. Manage licensing requirements and open source governance for use of software components. Distributing and sharing artifacts across an organization. System stability and reliability with high availability architecture. Smart search for binaries. Advanced maintenance and monitoring tools.  Cloud infrastructure has provided additional benefits. With the cloud, binary repositories can now:\n Enable replication and resiliency through the use of global data centers. Provide lower latency and improved network performance by being available closer to end-users. Provide their services at the edge of the network regionally and globally to edge devices. Utilize cloud storage for reduced costs, scalability and lower maintenance. Leverage cloud services such as security vulnerability databases to extend their functionality.  "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/55_workshop_setup/10_cloud9.html",
	"title": "Cloud9",
	"tags": [],
	"description": "",
	"content": " AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you don’t need to install files or configure your development machine to start new projects.\nAdd a new Cloud9 IDE environment 1 . Within the AWS console, use the region drop list to select us-west-2 (Oregon). We are doing this because the workshop script will provision the resources in this same region.\n2 . Navigate to the cloud9 console or just search for it under the AWS console services menu.\n3 . Click the Create environment button\n4 . For the name use jfrog-workshop\n5 . Select Other instance type and choose t3.medium\n6 . Leave all the other settings as default\nThis will take about 1-2 minutes to provision\n Configure Cloud9 IDE environment When the environment comes up, customize the environment by:\n1 . Close the welcome page tab\n2 . Close the lower work area tab\n3 . Open a new terminal tab in the main work area.\nYour workspace should now look like this and can hide the left hand environment explorer by clicking on the left side environment tab.\nIf you don\u0026rsquo;t like this dark theme, you can change it from the View / Themes Cloud9 workspace menu.\n Cloud9 requires third-party-cookies. You can whitelist the specific domains. You are having issues with this, Ad blockers, javascript disablers, and tracking blockers should be disabled for the cloud9 domain, or connecting to the workspace might be impacted.\n "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab/10_configure_github_integration.html",
	"title": "Configure the GitHub Integration",
	"tags": [],
	"description": "",
	"content": "In order for JFrog Pipelines to get access to the code in your awsworkshop repository, we must first set up a Pipelines GitHub integration. This allows Pipelines to authenticate and get access to your GitHub repositories. To do this, we create a GitHub Personal Access Token with the correct permissions.\nJFrog Pipelines can also integrate with other source code repositories such as GitHub Enterprise, BitBucket and GitLab.   Go to your GitHub Personal Access Tokens settings page. Click on Generate new token. Provide a name for the token. Configure the token for the following scopes.\n* repo (all) * admin:repo_hook (read, write) * admin:public_key (read, write) Click Generate token.\n Copy the token.\n Go to your JFrog Platform instance at https://[server name].jfrog.io. Refer to your JFrog Free Subscription Activation email if needed.\n Login to your JFrog Platform instance with your credentials.  Once logged into the environment, you will be presented with the landing page.  On the left sidebar menu, select Pipelines ► Integrations.  Click on Add an Integration.\n Give this integration the name GitHub.\n For the Integration Type, choose GitHub.\n Paste your GitHub Personal Access Token into the Token field.  Click Create.\n  You have created a GitHub Integration that allows JFrog Pipelines to access your GitHub repositories.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/10_devops_cloud.html",
	"title": "DevOps in the Cloud",
	"tags": [],
	"description": "",
	"content": "The goal of DevOps is to allow your development teams to deliver quality software faster to your customers through continuous process improvement, leveraging the best of breed development tools and infrastructure, and utilizing software development and IT operations best practices. Your team must deliver software faster than your competitors in order to get features and fixes to your customers sooner. JFrog terms this ideal as liquid software.\n Looking forward, as release cycles get shorter and microservices get smaller, we can imagine a world in which at any one time, our systems’ software is being updated. Effectively, software will become liquid in that products and services will be connected to “software pipes” that constantly stream updates into our systems and devices; liquid software continuously and automatically updating our systems with no human intervention.\n A critical aspect of DevOps is infrastructure. Cloud computing infrastructure has allowed DevOps to advance and come closer to realizing liquid software. Cloud computing has allowed development teams to build these software pipes by:\n Using ephemeral cloud infrastructure to scale their development process and software delivery at levels not achievable with on-premise infrastructure. Providing applications on a global scale with real-time response and resiliency. Leveraging new cloud services in their application and software development processes to improve the quality, security and delivery of their applications. Allowing multi-discipline teams to collaborate in the cloud across the software lifecycle to ensure quality, security, velocity and scale of applications.  "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/55_workshop_setup/self_paced.html",
	"title": "...on your own",
	"tags": [],
	"description": "",
	"content": " Running the workshop on your own Only complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Kubecon, Immersion Day, etc), go to Start the workshop at an AWS event.\n  An AWS account for access to Cloud9 IDE and AWS services - Create an AWS account.. A GitHub account for accessing and modifying workshop code - Create a GitHub account with these official instructions. Access to a JFrog Platform instance with Artifactory, Xray and Pipelines - Get your own JFrog Platform instance with the JFrog Platform Cloud Free Tier in just a few minutes.  \n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab/20_configure_artifactory_integration.html",
	"title": "Configure the Artifactory Integration",
	"tags": [],
	"description": "",
	"content": "Similar to the GitHub Integration, in the following steps you will configure an Artifactory Integration that allows JFrog Pipelines to also access your Artifactory repositories in order to publish your artifacts and build info. You will do this by creating an API key.\nArtifactory offers a universal solution supporting all major package formats including Alpine, Maven, Gradle, Docker, Conda, Conan, Debian, Go, Helm, Vagrant, YUM, P2, Ivy, NuGet, PHP, NPM, RubyGems, PyPI, Bower, CocoaPods, GitLFS, Opkg, SBT and more.   In your JFrog Platform instance, go your profile and select Edit Profile. Enter your password and click Unlock to edit the profile. In the Authentication Settings section, click the gear icon to generate an API key.  Copy the API key. Go back to Integrations, select Pipelines ► Integrations.  Click on Add an Integration. Give this integration the name Artifactory. For the Integration Type, choose Artifactory. Enter your JFrog Platform instance URL https://[server name].jfrog.io/artifactory for the url. Enter your username for the User. Paste your Artifactory API Key into the API Key field.  Click Create.  Remember this Artifactory username and API Key. We will use it again on the next step to set up ECS to deploy our image.  You have created an Artifactory Integration that allows JFrog Pipelines to access your Artifactory repositories. At this point, you should see the Artifactory and the GitHub Integrations in the Integrations list.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/10_devops_cloud/20_dev_sec_ops.html",
	"title": "DevSecOps",
	"tags": [],
	"description": "",
	"content": "Any security issue identified by a security scanning tool needs to be reviewed by a very small security team that may even lack the technical knowledge. This challenge can be reduced by shifting left to the developer and operations teams, making them also responsible for security and compliance, and moving security earlier in the software delivery process. Source code, dependency and artifact security scanning are some examples of moving security into the development process. Implementing the identification of security issues earlier in the CI/CD pipeline, as well as automating security and compliance policies in the Software Development Lifecycle (SDLC), rather than using manual processes, is crucial. Moreover, organizations that leave the Sec out of DevOps, may face security and compliance issues that are closer to their release, resulting in additional costs for remediating such issues.\nAs you move your SDLC to the cloud, your DevSecOps strategy must also adapt to the cloud. As discussed previously, binary repository managers that scale globally across cloud data centers require DevSecOps tools that will likewise scale and adjust. An enterprise scale software delivery system with multiple development teams, end users and devices mean more entry points for potential security and compliance issues. Therefore, it is critical that your SLDC is well-integrated with your DevSecOps system.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/20_jfrog_platform_overview.html",
	"title": "JFrog Platform Overview",
	"tags": [],
	"description": "",
	"content": "The JFrog Platform is designed to meet the growing needs of companies to develop and distribute software in the cloud. It provides DevOps with the tools needed to create, manage, secure and deploy software with ease. These tools cover everything from CI/CD orchestration, binary management, artifact maturity, security and vulnerability protection, release management, analytics and distribution.\nJFrog Artifactory is an Artifact Repository Manager that fully supports software packages created by any language or technology. Furthermore, it integrates with all major CI/CD and DevOps tools to provide an end-to-end, automated solution for tracking artifacts from development to production.\nJFrog Xray provides universal artifact analysis, increasing visibility and performance of your software components by recursively scanning all layers of your organization’s binary packages to provide radical transparency and unparalleled insight into your software architecture.\nJFrog Distribution empowers DevOps to distribute and continuously update remote locations with release-ready binaries.\nJFrog Artifactory Edge accelerates and provides control of release-ready binary distribution through a secure distributed network and edge nodes.\nJFrog Mission Control and Insight is your DevOps dashboard solution for managing multiple services of Artifactory, Xray, Edge and Distribution.\nJFrog Access with Federation provides governance to the distribution of artifacts by managing releases, permissions and access levels.\nJFrog Pipelines helps automate the non-human part of the whole software development process with continuous integration and empowers teams to implement the technical aspects of continuous delivery.\nAll of these JFrog Platform components are designed and developed to work together out-of-the-box with minimal configuration. Management and monitoring of your software delivery lifecycle from build to distribution is accessible though a central, unified user interface. The JFrog platform is enterprise ready with your choice of on-prem, cloud, multi-cloud or hybrid deployments that scale as you grow.\n "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab/30_configure_initialization_pipeline.html",
	"title": "Configure the Initialization Pipeline",
	"tags": [],
	"description": "",
	"content": "Next, we will update the lab pipelines to add your new GitHub and Artifactory integrations. In previous steps, you [forked and cloned the lab repository.]() We will modify the initialization pipeline in your forked Hoare repository to add these integrations.\n In your local git directory, open awsworkshop/jfrog_pipelines/init-jfrog.yml in an editor. Update the resources section of the file to use your new forked repository. Change the path to use your username.\nresources: - name: gitRepo_code type: GitRepo configuration: path: [your_Github_username]/awsworkshop \u0026lt;\u0026lt;\u0026lt;--- CHANGE HERE gitProvider: GitHub  Save your changes.\n In your terminal, git add, commit and push your changes.\n$ git add . $ git commit -m \u0026#39;Updated repository path.\u0026#39; $ git push Go to https://github.com/[username]/awsworkshop/blob/master/jfrog_pipelines/init-jfrog.yml and verify that your changes were made and pushed to your GitHub repository.\n  We are now ready to add and execute your pipelines with JFrog Pipelines.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/55_workshop_setup/30_iamrole.html",
	"title": "Create an IAM role for your workspace",
	"tags": [],
	"description": "",
	"content": " Starting from here, when you see command to be entered such as below, you will enter these commands into Cloud9 IDE. You can use the Copy to clipboard feature (right hand upper corner) to simply copy and paste into Cloud9. In order to paste, you can use Ctrl + V for Windows or Command + V for Mac.\n  Follow this deep link to create an IAM role with Administrator access. Confirm that AWS service and EC2 are selected, then click Next to view permissions. Confirm that AdministratorAccess is checked, then click Next to review. Enter Jfrog-Workshop-Admin for the Name, and select Create Role   "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/55_workshop_setup/40_workspaceiam.html",
	"title": "Attach the IAM role to your Workspace",
	"tags": [],
	"description": "",
	"content": " Follow this deep link to find your Cloud9 EC2 instance Select the instance, then choose Actions / Instance Settings / Attach/Replace IAM Role  Choose Jfrog-Workshop-Admin from the IAM Role drop down, and select Apply   "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab/40_configure_pipeline_source.html",
	"title": "Configure the Pipeline Source",
	"tags": [],
	"description": "",
	"content": "In these next steps, we will add the build pipelines as a JFrog Pipelines source. This will allow JFrog Pipelines to execute these pipelines automatically whenever there is a commit or manually as needed.\nA Pipeline Source represents a source control repository (such as GitHub or BitBucket) where Pipelines definition files can be found. A pipeline source connects to the repository through an integration.   In your JFrog Platform instance, go to Pipelines ► Pipeline Sources.  Click on Add Pipeline Source. Choose Single Branch. This is for repositories that only have one branch like master. For the Integration choose the GitHub Integration that you created previously named GitHub. For the Repository Full Name, specify your Hoare repository name in the form [username]/Hoare. Leave the Branch as master. For the Pipeline Config Filter, specify jfrog_pipelines/.*yml. Click Create.   It will take a few moments for JFrog Pipelines to sync the pipelines from your new Pipelines Source. During this time, JFrog Pipelines will load and process the pipelines for syntax, resources and dependencies. When complete, you should see a Success status. 9. Click on Logs on the right to view more details on the sync process. 10. Go back to Pipelines ► My Pipelines, and you will see your added pipelines. With your pipelines added, we are now ready to execute the pipelines, initialize our environment and build our artifacts!\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/55_workshop_setup/45_cloud.html",
	"title": "Update IAM settings for your Workspace",
	"tags": [],
	"description": "",
	"content": " Cloud9 normally manages IAM credentials dynamically. This isn\u0026rsquo;t currently compatible with the EKS IAM authentication, so we will disable it and rely on the IAM role instead.\n  Return to your workspace and click the gear icon (in top right corner), or click to open a new tab and choose \u0026ldquo;Open Preferences\u0026rdquo; Select AWS SETTINGS Turn off AWS managed temporary credentials Close the Preferences tab   Let\u0026rsquo;s run the command below, the following actions will take place as we do that:\n🔹 Install jq- jq is a command-line tool for parsing JSON\n🔹 Ensure temporary credentials aren’t already in place.\n🔹 Remove any existing credentials file.\n🔹 Set the region to work with our desired region.\n🔹 Validate that our IAM role is valid.\nsudo yum -y install jq rm -vf ${HOME}/.aws/credentials export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) test -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set echo \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region aws sts get-caller-identity --query Arn | grep Jfrog-Workshop-Admin -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; If the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab/50_execute_initialization_pipeline.html",
	"title": "Execute the Initialization Pipeline",
	"tags": [],
	"description": "",
	"content": "The first pipeline that we will execute will initialize our environment. This pipeline will create users, groups, permissions, repositories, Xray policies and watches, Xray indexes and access federation. This prepares our JFrog Platform instance to run our gradle and npm build pipelines.\nThis pipeline initializes the JFrog Platform for the next build pipelines by creating the necessary users, repositories, permissions and Xray configuration. It does this by using the JFrog Platform REST APIs. This is another way that you can manage and monitor the JFrog Platform.   Go to Pipelines ► My Pipelines.  Click on the init_jfrog pipeline in the Pipelines List. Click on the first step and further click on the trigger step icon to execute this pipeline. A run will appear, and it will take a few moments for JFrog Pipelines to allocate resources to execute the pipeline. The pipeline will take approximately 5 minutes to execute.  If the execution results in an error, click on the run to view the logs. Make any changes to the pipeline or integrations to correct any issues and then execute again.  The run will show a success status when it completes without errors.   We are now ready to build our first artifacts for our application.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/55_workshop_setup/55_jfrog_free.html",
	"title": "Get a Free JFrog Platform Instance",
	"tags": [],
	"description": "",
	"content": " Use the JFrog Platform Cloud Free Tier to get your own JFrog Platform instance with Artifactory, Xray and Pipelines.  \n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/55_workshop_setup.html",
	"title": "Workshop Setup",
	"tags": [],
	"description": "",
	"content": " Workshop Setup  AWS console Cloud9 IDE Environment JFrog Platform  Objectives of this section 🔹 Provision AWS Cloud9 IDE environment\n🔹 Provision JFrog Platform\n🔹 Run setup scripts to optimize your environment\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab/70_execute_npm_build_pipeline.html",
	"title": "Execute the NPM Build Pipeline",
	"tags": [],
	"description": "",
	"content": "The npm_build pipeline builds our web application. This pipeline uses a NpmBuild native Pipelines step build the user interface components. Next, it uses NpmPublish to publish the components. DockerBuild and DockerPush native steps are used to build a Docker image and push it to Artifactory. It then scans the build using the XrayScan native step. Then it pushes the produced artifacts to the \u0026ldquo;staging\u0026rdquo; repository in Artifactory along with all build information by using the PromoteBuild native step.\nA Step is a unit of execution in a pipeline. It is triggered by some event and uses resources to perform an action as part of the pipeline. Steps take Inputs in the form of Integrations or Resources, execute tasks that perform the operations necessary and then produce Outputs. These Outputs can become Inputs to other steps and so on forming a dependency-based, event-driven pipeline.   Steps are executed on build nodes. Dynamic build node pools are spun up and down on-demand by Pipelines from a cloud or Kubernetes service. This can help scale operations, and help manage costs by not incurring cloud service charges to run idle nodes. Static build node pools can also be used and are persistently available.   Go to Pipelines ► My Pipelines.  Click on the npm_build pipeline in the Pipelines List. Click on the View YAML icon to the right to view the pipeline steps discussed above.  Click on the Resources tab to view the details for all the resources that are used in our pipeline. We specify the relevant input and output resources such as Git repositories, build infos and file specs. These are referenced in our pipeline. Click on the Pipeline tab. The first step npm_prep is an NpmBuild step. This prepares the NPM environment for building. Additionally, the following occurs:\n The NPM repository is specified with repositoryName. Our Artifactory integration specifies our Artifactory server and inputResources is the same source code location.\n- name: npm_prep type: NpmBuild configuration: npmArgs: --no-progress --no-audit repositoryName: npm-demo sourceLocation: workshop-app integrations: - name: Artifactory inputResources: - name: gitRepo_code  The next step, npm_publish, uses NpmPublish to publish a package to the Artifactory repository npm-demo.\n- name: npm_publish type: NpmPublish configuration: repositoryName: npm-demo integrations: - name: Artifactory inputSteps: - name: npm_compile The npm_docker_build step executes a docker build. It does the following:\n onStart is used to execute bash commands prior to executing the step. Steps have additional lifecycle callbacks: onExecute, onSuccess, onFailure and onComplete. It will use the Docker file from the relevant location to generate the Docker image with a specific name and tag based on the value of the environment variables that were configured in the onStart. It will use the gitRepo_code GitRepo resource as an input resource to locate the Dockerfile. It will use the BuildInfo input resource from the previous step. It will create a docker image ${domain}/docker-demo/npm-app. domain is the JFrog Platform instance domain (\u0026lt;server\u0026gt;.jfrog.io).\n- name: npm_docker_build type: DockerBuild configuration: affinityGroup: bldGroup dockerFileLocation: workshop-app dockerFileName: Dockerfile dockerImageName: ${Fullimagename} dockerImageTag: ${Version} inputSteps: - name: npm_publish inputResources: - name: gitRepo_code integrations: - name: Artifactory execution: onStart: - pushd ${res_gitRepo_code_resourcePath}/workshop-app # Creating a Folder for the fileSpec Target - mkdir -p npm_results - popd - export domain=$(echo ${int_Artifactory_url} | awk -F[/:] \u0026#39;{print $4}\u0026#39; ) - export Fullimagename=\u0026#34;${domain}/docker-demo/npm-app\u0026#34; onSuccess: - echo \u0026#34;Congrats The Docker image was build\u0026#34;  The next step will push the Docker image to the target Docker repository in Artifactory.\n - name: Npm_docker_push type: DockerPush configuration: affinityGroup: bldGroup targetRepository: docker-demo autoPublishBuildInfo: true integrations: - name: Artifactory inputSteps: - name: npm_docker_build outputResources: - name: docker_npmBuild_Info The following step uses Xray to scan the docker image for security vulnerabilities and license compliance. By default, a failed Xray scan will result in a failure of the step and the pipeline.\n - name: npm_docker_scan type: XrayScan configuration: inputResources: - name: docker_npmBuild_Info trigger: true outputResources: - name: scanned_npm_dockerBuild_Info The last step will promote the build to the \u0026ldquo;staging\u0026rdquo; repository after passing the previous Xray scan.\n - name: npm_docker_promote type: PromoteBuild configuration: targetRepository: docker-demo-prod-local includeDependencies: true status: Passed comment: Artifact passed Xray Scan copy: false inputResources: - name: scanned_npm_dockerBuild_Info trigger: true outputResources: - name: final_docker_npmBuild_Info  Close the VIEW YAML window.\n Click on the first step and further click on the trigger step icon to execute this pipeline. It will take several minutes for this pipeline to run (~15-20 minutes).  When the run finishes successfully, switch to the Packages view in Artifactory. Go to Artifactory ► Packages.\n Type npm-app and search for the docker image that you just built.\n Then click on your docker npm-app listing.  This will show a list of the docker images for this build. Click on the latest version that you just built.  In the Xray Data tab, view the security violations.  Click on any violation to see the details and impact in the Issue Details tab.  Close the Issue Details tab.\n View the Docker configuration for the image in the Docker Layers tab.\n On the Builds tab, click on npm_build in the list.  Then click on your most recent build.\n In the Published Modules tab, view the set of artifacts and dependencies for your build.   The npm_build pipeline provided an overview of a typical build, docker build and push, security scan and promotion process using Artifactory, Pipelines and Xray. You were able to execute a pipeline, monitor the progress and examine its results. You explored new steps for NPM.\nNext, we will deploy your docker image from the \u0026ldquo;staging\u0026rdquo; repository using ECS.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab.html",
	"title": "Hands-On Lab",
	"tags": [],
	"description": "",
	"content": "In this tutorial, we will build a containerized NPM application. Using the JFrog Platform, we will compile our code, build our NPM package, execute a docker build and push, security scan the image and publish to a repository. We will then deploy the image and serve the application with Amazon ECS.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab/80_configure_ecs_permissions.html",
	"title": "Configure ECS Permissions",
	"tags": [],
	"description": "",
	"content": "In the previous section, we set up JFrog Pipelines to authenticate and publish images to Artifactory. In this section, we will add the same credentials to AWS Secrets Manager and create an ECS IAM role. This will allow Amazon ECS to pull the image from Artifactory and deploy it.\nPrivate registry authentication for ECS tasks using AWS Secrets Manager enables you to store your credentials securely and then reference them in your container definition. This allows your ECS tasks to use images from private repositories.   Go to your AWS Secrets Manager Console. Click on Store a new secret. Select Other type of secrets. Select the Plaintext format. And paste your Artifactory username and API Key.\n{ \u0026#34;username\u0026#34; : \u0026#34;\u0026lt;username\u0026gt;\u0026#34;, \u0026#34;password\u0026#34; : \u0026#34;\u0026lt;password\u0026gt;\u0026#34; } Click Next.\n Provide a Secret name like awsworkshop/jfrog-npm-app. Remember this name.\n Click Next.\n Leave the default settings on this next Configure automatic rotation page and click Next.\n On the Sample code page, click Store. You should now see your new secret listed.  Click on your new secret.\n Copy the Secret ARN.  Next, we must create an IAM role that allows ECS to access these credentials. Go to IAM Roles.\n Click on Create role.\n Select the Elastic Container Service service and Elastic Container Service Task use case.\n Click on Create Policy.\n Click on the JSON tab and paste the following.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;\u0026lt;Secret ARN\u0026gt;\u0026#34;, \u0026#34;arn:aws:kms:\u0026lt;region\u0026gt;:\u0026lt;aws_account_id\u0026gt;:key/key_id\u0026#34; ] } ] } Substitute your Secret ARN from above.\n Also substitute your \u0026lt;region\u0026gt; and \u0026lt;aws_account_id\u0026gt;. You can derive this from the Secret ARN format.\narn:aws:secretsmanager:\u0026lt;region\u0026gt;:\u0026lt;aws_account_id\u0026gt;: secret:secret_name Click on Review policy.\n Name the policy ecsAccessToSecrets and create the policy. This creates a policy that allows ECS to access your Artifactory credentials that are stored in the Secrets Manager.  Now go back to your role and search for your new policy ecsAccessToSecrets and attach it. You may need to refresh the policy list.\n Also attach the AmazonECSTaskExecutionRolePolicy. This policy allows the execution of Amazon ECS tasks.\n Click through the next steps and then create the role with the name ecsWorkshop.   You have now created an IAM role that will allow ECS to deploy images from Artifactory.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab/90_deploy_your_image.html",
	"title": "Deploy Your NPM Image with ECS",
	"tags": [],
	"description": "",
	"content": "We are now ready to deploy your image with Amazon ECS.\nPrivate registry authentication for ECS tasks using AWS Secrets Manager enables you to store your credentials securely and then reference them in your container definition. This allows your ECS tasks to use images from private repositories.   Go to the Amazon ECS console first-run wizard. In the Container definition section, click Configure on the custom option. For the container name, specify npm-app. For the Image specify the docker image name for your npm-app. This should be ${domain}/docker-demo/npm-app:latest. domain is the JFrog Platform instance domain (server.jfrog.io). Check Private repository autentication. For Secrets Manager ARN paste the Secrets ARN from the ECS permissions step. For port mapping, specify 443.  Click Update. Click Edit on the Task definition. for the Task definition name, specify deploy-npm-app. For the Task execution role specify the ECS role that you created, ecsWorkshop.  Click Save. Click Next. For Define your service, ensure Application Load Balancer is selected and port 443 is listed.  Click Next. For Configure your cluster, specify npm-app-cluster for your Cluster name. Click Next. Review your configuration. Click Create after you validated your configuration. Wait for your AWS services to be completed.  When ready, click on your deployed service.  Click on the Tasks tab.  Wait for the Last status to show RUNNING. Click on the deploy-npm-app task. On the Details page of the task, locate the Public IP.  In your browser, go to https://\u0026lt;Public IP\u0026gt; to view your deployed web application. Click through the self-signed certificate warning. You should see the following web application.  Congratulations! You have used ECS Fargate to deploy the image that you build with the JFrog Platform.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/70_hands-on_lab/100_conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "In this workshop, we demonstrated that using the JFrog Platform to create CI/CD pipelines to build an application, manage the artifacts, scan the artifacts for security vulnerabilities and license compliance, and publish the artifacts of your application to a staging repository. Then you used Amazon ECS to deploy your application so that end-users can access it. Now you have a basic understanding of the JFrog Platform as a modern DevOps cloud platform on AWS. We encourage you to use it with your existing build tools, like Maven, Gradle, Ivy and Ant and use it with packaging systems of other development platforms, like NuGet, RubyGems or Go.\n"
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/999_additional_resources.html",
	"title": "Additional Resources",
	"tags": [],
	"description": "",
	"content": " JFrog Platform Documentation - The full documentation of the JFrog Platform, the universal, hybrid, end-to-end DevOps automation solution. It is designed to take you through all the JFrog Products. Including user, administration and developer guides, installation and upgrade procedures, system architecture and configuration, and working with the JFrog application. JFrog Academy - Learn more about the JFrog Platform at your own pace with JFrog Academy free courses taught by our experts.  "
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://jefferyfry.github.io/awsworkshop/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]